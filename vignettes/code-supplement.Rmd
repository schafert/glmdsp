---
title: "code-supplement"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{code-supplement}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(glmdsp)
```

The following vignette provides an illustration of fitting the suite of models in the manuscript describing the proposed trend filtering model. The models are:

* Negative Binomial dynamic horseshoe (NB-DHS) available in *glmdsp*
* Negative Binomial independent horseshoe (NB-HS) available in *glmdsp*
* Exponential smoothing (Exp-Smooth) available in *forecast*
* Gaussian dynamic horseshoe available in *dspCP*
* Poisson fused lasso penalized regression (GLM-L1) available in *glmgen*

## Simulated time series

To simulate from the modified bumps functions, we can use `bumps_sim()` to construct the trend and `rnbinom()` to simulate the responses.

```{r}
set.seed(20230515)
library(ggplot2)
theme_set(theme_bw())

theta <- bumps_sim(n = 200)
y <- rnbinom(n = 200, size = 5, mu = theta)

ggplot()+
  geom_point(aes(x = 1:200, y = y))+
  geom_line(aes(x = 1:200, y = theta))
```

We will use the simulated responses `y` for each of the methods.

## Negative Binomial trend filtering

The Negative Binomial dynamic horseshoe model places a dependent prior on the increments and assumes a Negative Binomial data generating model. The shrinkage is applied to the differenced trend where the degree of differencing is a user choice. We can fit the model to the first and second differences.

```{r nb_dhs}
fit_nb_dhs <- btf_nb(y = y, 
                      D = 2, 
                      nburn = 5000,
                      evol0_sample = FALSE,
                      verbose = FALSE,
                      sigma_e = 1,
                      chol0 = TRUE
)
```

## Exponential smoothing

Exponential smoothing is a weighted average estimate where the current trend is an average of all of the past with exponential decay of the weights. Therefore, the more recent past has more influence on the current trend estimate.

```{r exp_smooth}
fit_exp_smooth <- forecast::ses(y) 
```

## Gaussian dynamic horseshoe

The dynamic shrinkage process for trend estimates using a Gaussian likelihood is provided by *dspCP* available on Github. 

```{r gau_dhs}
#devtools::install_github("haoxuanwu/dspCP")
library(dspCP)

fit_gau_dhs <- btf(y = y,
                    D = 2,
                    nburn = 5000,
                    verbose = FALSE
)
```

It is also a common approach in count data analysis to model the log transformation as Gaussian data. The following is an illustration of the transformation with an offset of 1 to accommodate zero counts.

```{r log_dhs}
fit_log_dhs <- btf(y = log(y + 1),
                    D = 2,
                    nburn = 5000,
                    verbose = FALSE
)
``` 

## Poisson fused LASSO

The one dimensional fused LASSO for trend filtering applies the LASSO penalty to the incremental differences and is most akin to the independent horseshoe penalty. It is available for the Poisson regression in the *glmgen* package. The degree of differencing in the fused LASSO is denoted by k where $k = D-1$.

```{r glm_l1}
# install_github("statsmaths/glmgen", subdir="R_pkg/glmgen")
library(glmgen) 

fit_glm <- trendfilter(y, 
                   k = 2-1,
                   family = "poisson")
```

To choose the penalty tuning parameter, I adapted the cross validation function from the *genlasso* package for Gaussian fused LASSO to work with the objects from *glmgen*.

```{r cv_glmgen}
cv_glmgen <- function (object, k = 5, mode = c("lambda", "df"), 
                       verbose = FALSE, ...) 
{
  cl = match.call()
  if (all(class(object) != "trendfilter")) {
    stop("Cross-validation can only be performed for trend filtering.")
  }
  if (!is.null(object$X)) {
    stop("Cross-validation for trend filtering can only be performed when X=I, the identity matrix.")
  }
  mode = mode[[1]]
  if (!(mode %in% c("lambda", "df"))) {
    stop("Invalid mode, must be \"lambda\" or \"df\".")
  }
  y = object$y
  n = length(y)
  if (k < 2 || round(k) != k || k > n - 2) {
    stop("The number of folds must an integer between 2 and n-2.")
  }
  ord = object$k ## glmgen has degree of differencing in the k slot
  pos = object$x
  if (is.null(pos)) 
    pos = 1:n
  foldid = c(0, rep(1:k, n - 2)[1:(n-2)], 0)
  if (mode == "lambda") {
    lambda = object$lambda
    cvall = matrix(0, k, length(lambda))
    for (i in 1:k) {
      cat(sprintf("Fold %i ... ", i))
      ## tr means train and te means test
      otr = which(foldid != i) ## indices of training for fold i
      ntr = length(otr)
      ytr = y[otr] ## observations for training for fold i
      ptr = pos[otr] ## positions for training for fold i
      wtr = object$weights[otr]
      out = glmgen::trendfilter(ptr, ytr, 
                                weights = wtr,
                                k = ord,
                                family = object$family,
                                lambda = lambda,
                                ...)
      # b = coef.genlasso(out, lambda = lambda)$beta
      
      ote = which(foldid == i)
      yte = matrix(y[ote], length(ote), length(lambda))
      pte = pos[ote]
      wte = object$weights[ote]

      pred = predict(out, type = "response",
                     x.new = pte)
      
      cvall[i, ] = colMeans((yte - pred)^2)
    }
    cverr = colMeans(cvall)
    cvse = apply(cvall, 2, sd)/sqrt(k)
    names(cverr) = names(cvse) = round(lambda, 3)
    i0 = which.min(cverr)
    lam.min = lambda[i0]
    lam.1se = max(lambda[cverr <= cverr[i0] + cvse[i0]])
    i.min = which(lambda == lam.min)
    i.1se = which(lambda == lam.1se)
    out = list(err = cverr, se = cvse, mode = "lambda", 
               lambda = lambda, lambda.min = lam.min, lambda.1se = lam.1se, 
               i.min = i.min, i.1se = i.1se, call = cl)
  }
  else {
    warning("method not available for mode df")
  }
  cat("\n")
  class(out) = c("cv.trendfilter", "list")
  return(out)
}
```


